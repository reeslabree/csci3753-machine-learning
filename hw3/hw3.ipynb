{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1555b0f7089adc531e263ee7e759eaaf",
     "grade": false,
     "grade_id": "cell-82ae56686993b52b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 3: Neural Networks\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Monday March 15**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/akkikiki/CSCI-4622-Machine-Learning-sp21/blob/main/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda (Version: 2019.07) with Python 3.7. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- In this homework, we will use [PyTorch](https://pytorch.org/) and PyTorch Lightning to implement a classifier. First upgrade your `pip` package manager to the latest version (Version >19.0). Then install the current stable release for [PyTorch](https://pytorch.org/get-started/locally/) (version: 1.6.0) as the backend for PyTorch Lightning. Then install [PyTorch Lighntning](https://pytorch-lightning.readthedocs.io/en/0.9.0/) (make sure to isntall version: 0.9.0 for this HW).\n",
    "\n",
    "```\n",
    "pip install --upgrade pip\n",
    "\n",
    "pip install torch\n",
    "\n",
    "pip install pytorch-lightning==0.9.0\n",
    "\n",
    "```\n",
    "\n",
    "It is **highly recommended** you install the CPU-only version of PyTorch if your'e unfamiliar with the process of installing CUDA libraries for these packages. If you choose to install a GPU version, ensure you're code runs without GPU support.\n",
    "\n",
    "**Acknowledgment** : Chris Ketelsen, Chenhao Tan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f62b9cdc5976a4fe6367164755ad2ebf",
     "grade": false,
     "grade_id": "cell-c45699879b420760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Please put your name and cuidentity username.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Rees LaBree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identity Key**: rela3138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test PyTorch installation\n",
    "import torch \n",
    "import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f61414c58e3b47da43407f606f8c4fa",
     "grade": false,
     "grade_id": "cell-d79b29ab197119c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[20 points] Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "### Part 1 [10 points] \n",
    "Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize the *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights, biases, and the *indicator* activation functions if applicable:\n",
    "\n",
    "- $\\; \\texttt{NOT } x_1$\n",
    "\n",
    "- $\\; x_1 \\texttt{ NAND } x_2$\n",
    "\n",
    "- $\\; x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "84fa84d70a98b9808ff01ecdd3ee6cfe",
     "grade": true,
     "grade_id": "cell-5504bb80827fe61b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "NOT and NAND linearly seperable which is why it can be learned by a single layer perceptron, XNOR is not and thus cannot.\n",
    "- NOT - weights = <-1>, bias = <0>, If wx+b > 0, y' = 1\n",
    "- NAND - weights = <-1, -1>, bias = <2>, If wx1+wx2+b > 0, y' = 1\n",
    "- XNOR could not be done with a single layer perceptron, it would require combinging multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0fd9c3fb6ac9ef00ed423489764ccfdc",
     "grade": false,
     "grade_id": "cell-34216deb363900f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 [10 points] \n",
    "\n",
    "Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. Make a truth table of $x_1$, $x_2$, and $x_1 \\texttt{ XNOR } x_2$, describe your perceptron's architecture, and state your weight matrices and bias vectors in Markdown below. Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "3dd94f21437a23eceec230c4f66dd863",
     "grade": true,
     "grade_id": "cell-fd1e475a5ef92def",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "XNOR is the same as an AND gate or'd with a NOR gate. Both AND and NOR can be accomplished with single layer perceptrons. As such we will feed b, w1x1 and w2x2 to AND and NOR, and then feed the product of the new weights and outputs and a new bias to OR to generate Y.\n",
    "- WEIGHTS: $w_{NOR} = <-1, -1>$, $w_{AND} = <1, 1>$, $w_{OR} = <2, 2>$\n",
    "- BIAS: $b_{AND} = b_{OR} = -1, b_{NOR} = 1$\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "x1& x2 & out\\\\ \\hline\n",
    "0 & 0 & 1\\\\ \\hline\n",
    "1 & 0 & 0\\\\ \\hline\n",
    "0 & 1 & 0\\\\ \\hline\n",
    "1 & 1 & 1\\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a95bc02f140db70f57390569aeb61fa2",
     "grade": true,
     "grade_id": "cell-330996afb5a81650",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x1 | x2 | out\n",
      "----+----+-----\n",
      "  0 |  0 |  1\n",
      "----+----+-----\n",
      "  0 |  1 |  0\n",
      "----+----+-----\n",
      "  1 |  0 |  0\n",
      "----+----+-----\n",
      "  1 |  1 |  1\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def XNOR(x1, x2):\n",
    "    w_AND = [1, 1]\n",
    "    w_NOR = [-1, -1]\n",
    "    w_OR = [2, 2]\n",
    "    bias_AND_OR = -1\n",
    "    bias_NOR = 1\n",
    "\n",
    "    AND = w_AND[0]*x1 + w_AND[1]*x2 + bias_AND_OR\n",
    "    if AND <= 0:\n",
    "        AND = 0\n",
    "    else:\n",
    "        AND = 1\n",
    "    \n",
    "    NOR = w_NOR[0]*x1 + w_NOR[1]*x2 + bias_NOR\n",
    "    if NOR <= 0:\n",
    "        NOR = 0\n",
    "    else:\n",
    "        NOR = 1\n",
    "    \n",
    "    result = w_OR[0]*AND + w_OR[1]*NOR + bias_AND_OR\n",
    "    if result <= 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "a = XNOR(0, 0)\n",
    "b = XNOR(0, 1)\n",
    "c = XNOR(1, 0)\n",
    "d = XNOR(1, 1)\n",
    "\n",
    "print(\" x1 | x2 | out\")\n",
    "print(\"----+----+-----\")\n",
    "print(\"  0 |  0 |  %d\" % (a))\n",
    "print(\"----+----+-----\")\n",
    "print(\"  0 |  1 |  %d\" % (b))\n",
    "print(\"----+----+-----\")\n",
    "print(\"  1 |  0 |  %d\" % (c))\n",
    "print(\"----+----+-----\")\n",
    "print(\"  1 |  1 |  %d\" % (d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9331334251af83bcd546a096b90eac7",
     "grade": false,
     "grade_id": "cell-410d0399fa830fab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[25 points] Problem 2 - Back propagation\n",
    "---\n",
    "\n",
    "In this problem you will gain some intuition about why training deep neural networks can be very time consuming.  Consider training a chain-like neural network: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^3) = \\frac{1}{2}(y - a^3)^2  \n",
    "$$\n",
    "\n",
    "where $a^3$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8c9f47b89404edf8e682707be900291",
     "grade": false,
     "grade_id": "cell-4337ff6b53b1760a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 1 [5 points]\n",
    "Suppose each of the weights is initialized to $W^k = -1.0$ and each bias is initialized to $b^k = 0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ed8ae08caeac509ef436f5d18c8223c",
     "grade": true,
     "grade_id": "cell-6512c42fc5e9ce1b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "- $a^0 = x = .5$\n",
    "- $z^1 = -1.0a^0+0.5 = -1.0 (.5) + 0.5 = 0$\n",
    "- $a^1 = \\text{sigmoid}(z^1) = .5$\n",
    "- $\\ell(y, a^1) = \\frac{1}{2}(y-a^1)^2 = \\frac{1}{8}$\n",
    "$$$$\n",
    "$$$$\n",
    "- $a^1 = .5$\n",
    "- $z^2 = -1.0a^1+0.5 = -1.0 (.5) + 0.5 = 0$\n",
    "- $a^2 = \\text{sigmoid}(z^1) = .5$\n",
    "- $\\ell(y, a^2) = \\frac{1}{2}(y-a^2)^2 = \\frac{1}{8}$\n",
    "$$$$\n",
    "$$$$\n",
    "- $a^2 = .5$\n",
    "- $z^3 = -1.0a^2+0.5 = -1.0 (.5) + 0.5 = 0$\n",
    "- $a^3 = \\text{sigmoid}(z^3) = .5$\n",
    "- $\\ell(y, a^3) = \\frac{1}{2}(y-a^3)^2 = \\frac{1}{8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a30f5d6c6d44bf28c59d9456bd916e2d",
     "grade": false,
     "grade_id": "cell-77928f9f19f6876f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 [5 points]:\n",
    "Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d95b2a0dc16f45336d915205d119763a",
     "grade": true,
     "grade_id": "cell-6a3b895fa888e925",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$\n",
    "\\delta^3 = \\frac{\\partial \\ell}{\\partial a^L_j} \\cdot g'(z^3) = (a^3 - y) \\sigma '(z^3) = \\frac{1}{2}\\sigma '(z^3)\\\\\n",
    "$\n",
    "$$$$\n",
    "for K = 3\n",
    "$\n",
    "\\frac{\\partial \\ell}{\\partial W^3} = \\delta^3(a^2)^T = \\frac{1}{2}\\sigma '(z^3)\\cdot \\frac{1}{2} = \\frac{1}{4}\\sigma '(z^3) \\Rightarrow \\boxed{\\frac{1}{16}}\\\\\n",
    "\\frac{\\partial \\ell}{\\partial b^\\mathcal{k}} = \\delta^3 = \\frac{1}{2}\\sigma '(z^3)\\Rightarrow\\boxed{\\frac{1}{8}}\\\\\n",
    "$\n",
    "$$$$\n",
    "for K = 2\n",
    "$\n",
    "\\delta^2 = (W^\\ell)^T\\delta^3\\cdot g'(z^2) = -1\\cdot\\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\\\\n",
    "\\frac{\\partial \\ell}{\\partial W^2} = \\delta^2(a^1)^T = -\\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\cdot(\\frac{1}{2}) = -\\frac{1}{4}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\Rightarrow\\boxed{-\\frac{1}{64}}\\\\\n",
    "\\frac{\\partial \\ell}{\\partial b^\\mathcal{k}} = \\delta^2 = -\\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2) \\Rightarrow \\boxed{-\\frac{1}{32}}\\\\\n",
    "$\n",
    "$$$$\n",
    "for K = 1\n",
    "$\n",
    "\\delta^1 = (W^\\ell)^T\\delta^2\\cdot g'(z^1) = \\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\cdot \\sigma'(z^1)\\\\\n",
    "\\frac{\\partial \\ell}{\\partial W^2} = \\delta^2(a^1)^T = \\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\cdot \\sigma'(z^1)\\cdot(\\frac{1}{2}) = \\frac{1}{4}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\cdot \\sigma'(z^1)\\Rightarrow\\boxed{\\frac{1}{256}}\\\\\n",
    "\\frac{\\partial \\ell}{\\partial b^\\mathcal{k}} = \\delta^2 = \\frac{1}{2}\\sigma '(z^3)\\cdot \\sigma'(z^2)\\cdot \\sigma'(z^1)\\Rightarrow\\boxed{\\frac{1}{128}}\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "768509621e57448e39d97e333e719201",
     "grade": false,
     "grade_id": "cell-161b07ac7c31e3c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 3 [5 points]\n",
    "Implement the following activation functions in NumPy:\n",
    "* ReLU\n",
    "* Sigmoid\n",
    "* softmax\n",
    "Please **do not use any other external libraries** for this such as sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3095d722bd4a8f2fdf5aaca330d63aa",
     "grade": false,
     "grade_id": "cell-3b7e3adaffe2c2ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # YOUR CODE HERE\n",
    "    return max(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    # YOUR CODE HERE\n",
    "    return (1/1+np.exp(-1*x))\n",
    "\n",
    "def softmax(x):\n",
    "    # YOUR CODE HERE\n",
    "    sum_x = 0\n",
    "    for x_i in x:\n",
    "        sum_x += x_i\n",
    "    return (np.exp(x)/sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a8ff3566873a584f28b89bd21c0689e6",
     "grade": true,
     "grade_id": "cell-0bded3752d3226b9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TestRelu (tests.tests.TestActiv) ... ok\n",
      "TestSigmoid (tests.tests.TestActiv) ... ok\n",
      "TestSoftmax (tests.tests.TestActiv) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# random tests for checking your impementation.\n",
    "from tests import tests\n",
    "tests.run_test_suite('prob 2.3', (relu, sigmoid, softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a185c9a75c41d12ed1c797a0592301a9",
     "grade": false,
     "grade_id": "cell-f97c1b3bf8c79803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 4 [5 points] \n",
    "Implement the following Loss functions in NumPy:\n",
    "* mean squared error\n",
    "* mean absolute error\n",
    "* hinge\n",
    "\n",
    "Please **do not use any other external libraries** for this such as sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "82e56ad803e5c2c632c83373c33c163e",
     "grade": false,
     "grade_id": "cell-84456d343bfeca31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat, y):\n",
    "    # YOUR CODE HERE\n",
    "    sum_i = 0.0\n",
    "    for i, j in enumerate(yhat):\n",
    "        sum_i += ((y[i]-j)**2)\n",
    "    \n",
    "    sum_i = sum_i / len(y)\n",
    "    return sum_i\n",
    "    \n",
    "def mean_absolute_error(yhat, y):\n",
    "    sum_i = 0.0\n",
    "    for i, j in enumerate(yhat):\n",
    "        sum_i += abs(y[i]-j)\n",
    "    \n",
    "    sum_i = sum_i / len(y)\n",
    "    return sum_i\n",
    "\n",
    "def hinge(yhat, y):\n",
    "    # YOUR CODE HERE\n",
    "    sum_i = 0\n",
    "    for i, j in enumerate(yhat):\n",
    "        sum_i += max(0.0, (1-y[i]*j))\n",
    "        \n",
    "    sum_i = sum_i / len(y)\n",
    "    return sum_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19c1adbc6aedcb50386631e966db71a3",
     "grade": true,
     "grade_id": "cell-550e920d814cb6d3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TestMSE (tests.tests.TestLoss) ... ok\n",
      "TestMAE (tests.tests.TestLoss) ... ok\n",
      "TestHinge (tests.tests.TestLoss) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.023s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# random tests for checking your impementation.\n",
    "from tests import tests\n",
    "tests.run_test_suite('prob 2.4', (mean_squared_error, mean_absolute_error, hinge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "406a8e23787fe614a2489ada61b0cdbb",
     "grade": false,
     "grade_id": "cell-e4c6962049a18e97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 5 [5 points]\n",
    "Explain the vanishing gradient problem. When would you observe this? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "550c7233d3dc395aa4965b23ce1126a4",
     "grade": true,
     "grade_id": "cell-59ccf91056b5d8bb",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "By compressing the output of a gradient based approach to neural networks using certain activation functions (i.e. reducing to the range [0,1]), large changes in the data will be compressed to small, indiscernable changes and hence the gradient \"vanishes\". You would observe this when the neural networks are initialized with excessively large negative or positive weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "80c63b94db61c0e3c652b020a8cc1ff8",
     "grade": false,
     "grade_id": "cell-85519893ca971e10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[23 Points] Problem 3 - Build a feedforward neural network in NumPy\n",
    "---\n",
    "\n",
    "In this problem you will implement a class representing a general feed-forward neural network that utilizes the sigmoid activation functions. Your tasks will be to implement forward propagation, prediction, back propagation, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "\n",
    "The skeleton for the network class is below. Note that this class is almost identical to the one you worked with in the \"hands-on neural network\" in-class notebook, so you should look at there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests. As with the previous problem, nothing in this problem should use any external libraries apart from numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d5435e7587f8c97f354a1242ab8e1cbc",
     "grade": false,
     "grade_id": "cell-b8abc0ac570aac74",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes, keep_prob=-1):\n",
    "        \"\"\"\n",
    "        sizes is a list : [input_dimensions, hidden_layer_dimensions, output_dimensions]\n",
    "        \"\"\"\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(n, m) for (\n",
    "            m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.keep_prob = keep_prob\n",
    "        self.acc_train_array = []\n",
    "        self.acc_test_array = []\n",
    "\n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        activation function\n",
    "        \"\"\"\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of activation function\n",
    "        \"\"\"\n",
    "        return sigmoid_prime(z)\n",
    "\n",
    "    def forward_prop(self, a):\n",
    "        \"\"\"\n",
    "        memory aware forward propagation for testing\n",
    "        only.  back_prop implements it's own forward_prop\n",
    "        \"\"\"\n",
    "        #YOUR CODE HERE\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)] \n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            a = self.g(z)\n",
    "            a_list.append(a)\n",
    "        \n",
    "    def grad_cost(self, a, y):\n",
    "        \"\"\"\n",
    "        gradient of cost function\n",
    "        Assumes C(a,y) = (a-y)^2/2\n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def log_train_progress(self, train, test, epoch):\n",
    "        \"\"\" Logs training progres. \n",
    "        \"\"\"\n",
    "        acc_train = self.evaluate(train)\n",
    "        self.acc_train_array.append(acc_train)\n",
    "        if test is not None:\n",
    "            acc_test = self.evaluate(test)\n",
    "            self.acc_test_array.append(acc_test)\n",
    "            print(\"Epoch {:4d}: Train {:10.5f}, Test {:10.5f}\".format(\n",
    "                epoch+1, acc_train, acc_test))\n",
    "        else:\n",
    "            print(\"Epoch {:4d}: Train {:10.5f}\".format(\n",
    "                epoch+1, acc_train))\n",
    "            \n",
    "\n",
    "    def SGD_train(self, train, epochs, eta, lam=0.0, verbose=True, test=None):\n",
    "        \"\"\"\n",
    "        SGD for training parameters\n",
    "        epochs is the number of epocs to run\n",
    "        eta is the learning rate\n",
    "        lam is the regularization parameter\n",
    "        If verbose is set will print progressive accuracy updates\n",
    "        If test set is provided, routine will print accuracy on test set as learning evolves\n",
    "        \"\"\"\n",
    "        n_train = len(train)\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(n_train)\n",
    "            for kk in range(n_train):\n",
    "                self.SGD_step(*train[perm[kk]], eta, lam)\n",
    "            if verbose and epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "                self.log_train_progress(train, test, epoch)\n",
    "    \n",
    "    def SGD_step(self, x, y, eta, lam):\n",
    "        \"\"\"\n",
    "        TODO: get gradients with x, y and do SGD on weights and biases\n",
    "\n",
    "        Args:\n",
    "            x: single sample features.\n",
    "            y: single sample target.\n",
    "            eta: learning rate.\n",
    "            lam: Regularization parameter.\n",
    "                \n",
    "        \"\"\"\n",
    "        # TODO: get gradients with xk, yk and do SGD on weights and biases\n",
    "        # YOUR CODE HERE\n",
    "        # w'=w-ndL/dWk\n",
    "        (dW_list, db_list) = back_prop(x, y)\n",
    "        self.weights\n",
    "        \n",
    "\n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation for derivatives of C wrt parameters\n",
    "        \"\"\"\n",
    "        db_list = [np.zeros(b.shape) for b in self.biases]\n",
    "        dW_list = [np.zeros(W.shape) for W in self.weights]\n",
    "        \n",
    "        a = x\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)]  # Pad with a placeholder so that indices match\n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(W, a) + b\n",
    "            z_list.append(z)\n",
    "            a = self.g(z)\n",
    "            a_list.append(a)\n",
    "\n",
    "        # Back propagate deltas to compute derivatives\n",
    "        # The following list gives hints on how to do it\n",
    "        # calculating delta (Error) for the output layer\n",
    "        # for the appropriate layers compute db_list[ell], dW_list[ell], delta\n",
    "        delta_k = a_list[len(self.L -1)]\n",
    "        for l in range(len(self.L)-1, 0, -1):\n",
    "            dW_list[l] = np.dot(delta_k, np.transpose(a_list[l-1]))\n",
    "            db_list[l] = delta_k\n",
    "            delta_k = np.dot(np.transpose(self.weights[l]), g_prime(z_list[l-1]))\n",
    "        \n",
    "        return (dW_list, db_list)\n",
    "    \n",
    "    def back_prop_dropout(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation with dropout on the hidden layers other than the output layer.\n",
    "        \n",
    "        Dropout layer can be thought of as a special linear layer between layers.\n",
    "        \"\"\"\n",
    "        db_list = [np.zeros(b.shape) for b in self.biases]\n",
    "        dW_list = [np.zeros(W.shape) for W in self.weights]\n",
    "        \n",
    "        a = x\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)]  # Pad with a placeholder so that indices match\n",
    "        # TODO: implement dropout using self.keep_prob\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return (dW_list, db_list)\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        \"\"\"\n",
    "        Evaluate current model on labeled test data\n",
    "        \"\"\"\n",
    "        ctr = 0\n",
    "        for x, y in test:\n",
    "            yhat = self.forward_prop(x)\n",
    "            ctr += yhat.argmax() == y.argmax()\n",
    "        return float(ctr) / float(len(test))\n",
    "\n",
    "\n",
    "def sigmoid(z, threshold=20):\n",
    "    z = np.clip(z, -threshold, threshold)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "\n",
    "def mnist_digit_show(flatimage, outname=None):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    image = np.reshape(flatimage, (-1, 14))\n",
    "\n",
    "    plt.matshow(image, cmap=plt.cm.binary)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if outname:\n",
    "        plt.savefig(outname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6ffe07da5b07c9f579a0ce82b68ec614",
     "grade": false,
     "grade_id": "cell-7eed131bbb859104",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 1 [15 points]\n",
    "Implement `SGD_step`, `back_prop`, and `forward_prop`. Use the following test cases to verify if the code is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1d6d5f4292a5c37a83b510f0084a5920",
     "grade": true,
     "grade_id": "cell-7632a78793a2588e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TestBackPropWithoutRegularization (tests.tests.TestNetwork) ... ERROR\n",
      "TestBackPropWithRegularization (tests.tests.TestNetwork) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: TestBackPropWithoutRegularization (tests.tests.TestNetwork)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\reesl\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\", line 45, in TestBackPropWithoutRegularization\n",
      "    nn_noreg.SGD_train(self.train, epochs=5, eta=0.25, lam=0.0, verbose=False)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 82, in SGD_train\n",
      "    self.SGD_step(*train[perm[kk]], eta, lam)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 100, in SGD_step\n",
      "    (dW_list, db_list) = back_prop(x, y)\n",
      "NameError: name 'back_prop' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: TestBackPropWithRegularization (tests.tests.TestNetwork)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\reesl\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\", line 75, in TestBackPropWithRegularization\n",
      "    nn_reg.SGD_train(self.train, epochs=5, eta=0.25, lam=0.2, verbose=False)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 82, in SGD_train\n",
      "    self.SGD_step(*train[perm[kk]], eta, lam)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 100, in SGD_step\n",
      "    (dW_list, db_list) = back_prop(x, y)\n",
      "NameError: name 'back_prop' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.075s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "one or more tests for prob 3 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-afb66815342e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtests\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test_suite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prob 3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\u001b[0m in \u001b[0;36mrun_test_suite\u001b[1;34m(name, ctor)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"TestBackPropWithoutRegularization\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TestBackPropWithRegularization\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mprob3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         assert unittest.TextTestRunner(verbosity=2).run(\n\u001b[0m\u001b[0;32m    259\u001b[0m             prob3).wasSuccessful(), \"one or more tests for prob 3 failed\"\n\u001b[0;32m    260\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"prob 4\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: one or more tests for prob 3 failed"
     ]
    }
   ],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 3', Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25edf8e016e56588ee1d7b22ce93a10b",
     "grade": false,
     "grade_id": "cell-3a21b0c7d363da0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 [8 points]\n",
    "\n",
    "Run the above Network on MNIST Dataset and report the following (feel free to experiment with different learning rates).\n",
    "\n",
    "* Change the hidden layer dimensions and experiment with these values: [5, 10, 20].\n",
    "* Plot accuracies of different hidden layer dimensions vs. epochs for both training and testing.\n",
    "* Explain the effect of hidden layer dimension on performance. \n",
    "\n",
    "**Note:** Accuracies are stored in `self.acc_train_array` and `self.acc_test_array` if `verbose` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './data/tinyMNIST.pkl.gz'\n",
    "f = gzip.open(location, 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "train, test = u.load()\n",
    "input_dimensions = len(train[0][0])\n",
    "output_dimensions = len(train[0][1])\n",
    "print('Number of Input Features: ', input_dimensions)\n",
    "print('Number of Output classes: ', output_dimensions)\n",
    "\n",
    "nns = []\n",
    "for hidden_layer_dimensions in [5, 10, 20]:\n",
    "    print('\\nHidden Layer Dimensions: ', hidden_layer_dimensions)\n",
    "    nn = Network([input_dimensions, hidden_layer_dimensions, output_dimensions])\n",
    "    nns.append(nn)\n",
    "    nn.SGD_train(train, epochs=200, eta=0.1, lam=0.0001, verbose=True, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f93da7a1fb1a534fdba7fccbcacef6c2",
     "grade": false,
     "grade_id": "cell-3a4d12fce7f9f7c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot training and testing accuracies below for each configuration. Use solid lines for testing accuracy, and dotted lines for train accuracy (`ls='dashed`). \n",
    "\n",
    "Be sure to label your lines and match colors accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "840bc3dd524f998267f04c6ec2eaa2cb",
     "grade": true,
     "grade_id": "cell-5e86a6c1f3a99b62",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\n",
    "epochs_array = [i for i in range(1, 220, 20)]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9cdbb2067c8b35fc50d7701498c5a636",
     "grade": false,
     "grade_id": "cell-ce9008bef635107d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Give your explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c4e73d7bb51d5e82e745832d85ed6aa2",
     "grade": true,
     "grade_id": "cell-fb5c5d0fd9a6d900",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e87adf1174c1dd47be5f7caefdbe28d6",
     "grade": false,
     "grade_id": "cell-460dc5490c1516b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Extra Credit [10 points]:** Implement dropout by filling the `back_prop_dropout` function and update the `SGD_train` function to use it. Explain the impact of dropout on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 4', Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_dimensions = 60\n",
    "nn = Network([input_dimensions, hidden_layer_dimensions, output_dimensions], keep_prob=0.5)\n",
    "nn.SGD_train(train, epochs=400, eta=0.1, lam=0, verbose=True, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d1f81c6e069ad24bed4034816ccc6c65",
     "grade": true,
     "grade_id": "cell-f7345360536d4c03",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40fa9ee6a8bd8ccd5a44d30f72afaa4a",
     "grade": false,
     "grade_id": "cell-65d25b85ad55bfe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [9 points] Problem 4 - Tensors and Autograd.\n",
    "In this problem, we will demonstrate the benefits of PyTorch's Autograd mechanics by converting your implementation from Problem to utilize PyTorch Tensors. This problem is not meant to be difficult, rather it should be quite simple once you've done Problem 3.\n",
    "\n",
    "Note that there are *better* ways to go about this which we will cover next. Here. we are keeping things as close to numpy as possible on purpose to show what's going on behind the scenes. Take a moment to look through our modifications to get an understanding of the differences. In the comments, we'll make notes of native torch functions that perform the same operations. \n",
    "\n",
    "Here, SGD is implemented manually to show you what's going on behind the scenes, but  usually you'd use [`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD). For future reference, the `weight_decay` parameter implements L2 normilazion. \n",
    "\n",
    "--- \n",
    "\n",
    "Before we move forward, let's cover a couple of points on notation. \n",
    "\n",
    "Just as it's best practice to do `import numpy as np`, PyTorch has best practices for imports as well:\n",
    "\n",
    "```python\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "```\n",
    "You'll find that many of the functions you've been learning in NumPy have direct translations in PyTorch. The documentation can be found [here](https://pytorch.org/docs/stable/index.html) and is very user friendly, but we'll cover a few notable differences. \n",
    "\n",
    "- The numpy parameter `axis` becomes `dim` in PyTorch.\n",
    "- Tensors have [view](https://pytorch.org/docs/stable/tensor_view.html) operations, which avoid copying data unnecasarily. Use these instead of `reshape()`. \n",
    "- Using `torch.from_numpy()` also avoids copying data stored in `numpy` arrays.\n",
    "- Numpy interprets `np.dot()` of 2D matrices as matrix multiplication. Pytorch does not have this behaviour, see instead [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)\n",
    "\n",
    "---\n",
    "Now onto the problem, your tasks are as follows:\n",
    "\n",
    "1. Modify `forward_prop` to work on PyTorch Tensors.\n",
    "2. Modify `SGD_step` to work on PyTorch Tensors. This is easier than it sounds, since with PyTorch we don't have to worry about calculating gradients manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7eeaf2c0b87ca9bda0129de263370e3b",
     "grade": false,
     "grade_id": "cell-f503b4357e8d35fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TorchNetwork(Network):\n",
    "    def __init__(self, sizes, keep_prob=-1):\n",
    "        super().__init__(sizes, keep_prob)\n",
    "        \n",
    "        # Convert your weights and biases to Tensors.\n",
    "        # The flag requires_grad=True tells PyTorch to track gradients for us. \n",
    "        self.biases = [torch.tensor(b, requires_grad=True) for b in self.biases]\n",
    "        self.weights = [torch.tensor(w, requires_grad=True) for w in self.weights]\n",
    "        \n",
    "\n",
    "    def g(self, z):\n",
    "        \"\"\" Activation function\n",
    "            Could also use torch.sigmoid(z)\n",
    "        \"\"\"\n",
    "        return sigmoid_torch(z) \n",
    "\n",
    "    def forward_prop(self, a):\n",
    "        \"\"\" Memory aware forward propagation. \n",
    "            Now this is for both training and testing.\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO: Convert your forward_prop function from part a to work on Tensors. \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def cost(self, a, y):\n",
    "        \"\"\" Cost function, same as F.mse_loss()\n",
    "        \"\"\"\n",
    "        return ((a - y)**2).mean() \n",
    "            \n",
    "\n",
    "    def SGD_step(self, x, y, eta, lam):\n",
    "        \"\"\"\n",
    "            Performs a single step of SGD on weights and biases\n",
    "\n",
    "        Args:\n",
    "            x: single sample features.\n",
    "            y: single sample target.\n",
    "            eta: learning rate.\n",
    "            lam: Regularization parameter.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Zero out the old gradients if they exist. \n",
    "        # Would be optim.zero_grad() if we were using an optimizer.\n",
    "        for p in (*self.weights, *self.biases):\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "            \n",
    "        # TODO: \n",
    "        # (1) call forward_prop to get a. \n",
    "        # (2) Calucale loss using the cost function.\n",
    "        # (3) Use loss.backward() to to perform backpropagation automatically.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Perform sgd update \n",
    "        # Would be optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            # l2 regularization on weights only.\n",
    "            for p in self.weights:\n",
    "                p.grad.add_(p, alpha=lam)\n",
    "            # sgd update. \n",
    "            for p in (*self.weights, *self.biases):\n",
    "                p.add_(p.grad, alpha=-eta)\n",
    "                \n",
    "\n",
    "def sigmoid_torch(z, threshold=20):\n",
    "    \"\"\" Same as sigmoid before, now with torch.clamp() instead of np.clip().\n",
    "        We could also just use torch.sigmoid().\n",
    "    \"\"\"\n",
    "    z = torch.clamp(z, -threshold, threshold)\n",
    "    return 1.0 / (1.0 + torch.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1206b00e94aeffbf21002354917dd1c1",
     "grade": false,
     "grade_id": "cell-6227665a74ae9306",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part 5 [10 points]:** Implement `SGD_step` and `forward_prop`. Use the following test cases to verify if the code is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8063cda31e787286f6869547bdb1ae30",
     "grade": true,
     "grade_id": "cell-9b1a944152b5097c",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TestBackPropWithoutRegularization (tests.tests.TestNetwork) ... ERROR\n",
      "TestBackPropWithRegularization (tests.tests.TestNetwork) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: TestBackPropWithoutRegularization (tests.tests.TestNetwork)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\reesl\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\", line 45, in TestBackPropWithoutRegularization\n",
      "    nn_noreg.SGD_train(self.train, epochs=5, eta=0.25, lam=0.0, verbose=False)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 82, in SGD_train\n",
      "    self.SGD_step(*train[perm[kk]], eta, lam)\n",
      "  File \"<ipython-input-9-183b3d232823>\", line 60, in SGD_step\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n",
      "\n",
      "======================================================================\n",
      "ERROR: TestBackPropWithRegularization (tests.tests.TestNetwork)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\reesl\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\", line 75, in TestBackPropWithRegularization\n",
      "    nn_reg.SGD_train(self.train, epochs=5, eta=0.25, lam=0.2, verbose=False)\n",
      "  File \"<ipython-input-7-cc4db12e3345>\", line 82, in SGD_train\n",
      "    self.SGD_step(*train[perm[kk]], eta, lam)\n",
      "  File \"<ipython-input-9-183b3d232823>\", line 60, in SGD_step\n",
      "    raise NotImplementedError()\n",
      "NotImplementedError\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.059s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "one or more tests for prob 5 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5b13af0c6681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtests\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_test_suite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prob 5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTorchNetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Dropbox\\School\\csci4622\\hw3\\tests\\tests.py\u001b[0m in \u001b[0;36mrun_test_suite\u001b[1;34m(name, ctor)\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \"TestBackPropWithoutRegularization\", \"TestBackPropWithRegularization\"]:\n\u001b[0;32m    271\u001b[0m             \u001b[0mprob5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         assert unittest.TextTestRunner(verbosity=2).run(\n\u001b[0m\u001b[0;32m    273\u001b[0m             prob5).wasSuccessful(), \"one or more tests for prob 5 failed\"\n\u001b[0;32m    274\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: one or more tests for prob 5 failed"
     ]
    }
   ],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 5', TorchNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f61c9ec925daa8bf8156b744a5e1890c",
     "grade": false,
     "grade_id": "cell-895751c3d6ccf24a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[23 Points] Problem 5 - Implement RNN Network to classify whether text is spam or ham \n",
    "---\n",
    "\n",
    "Dataset is obtained from UCI Machine Learning repository consisting of SMS tagged messages (labelled as either **ham** (legitimate) or **spam**) that have been collected for SMS Spam research.\n",
    "\n",
    "We will now use [PyTorch](https://pytorch.org/docs/stable/index.html) to implement a classifier. Update the snippet below to build a model with an embedding layer, and an LSTM layer, and a dense layer. This question allows you to get familiar with popular deep learning toolkits and the solution only has a few lines. In practice, there is no need to reinvent the wheels.\n",
    "\n",
    "\n",
    "Learn more about RNN : https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "**Note**: You need to install Tensorflow for this problem (CPU version if you do not have a nvidia GPU). You can find installation instructions [here](https://www.tensorflow.org/install)\n",
    "\n",
    "If you are using Anaconda, install the CPU version by doing \n",
    "```\n",
    "conda install 'tensorflow=*=mkl*'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "class SpamHam(pl.LightningDataModule):\n",
    "    \"\"\" A datamodule for the RNN. You shouldn't have to modify this class.\"\"\"\n",
    "    def __init__(self, dict_size=5000, example_length=150, batch_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dict_size = dict_size\n",
    "        self.batch_size = batch_size\n",
    "        self.example_length = example_length\n",
    "\n",
    "        # preprocess training data\n",
    "        self.tok = Tokenizer(num_words=dict_size)\n",
    "\n",
    "    def load_data(self, location):\n",
    "        return pickle.load(open(location, 'rb'))\n",
    "\n",
    "    def make_dataset(self, x, y, tok=None):\n",
    "        # tokenize\n",
    "        sequences = self.tok.texts_to_sequences(x)\n",
    "        x = sequence.pad_sequences(sequences, maxlen=self.example_length)\n",
    "\n",
    "        # make torch arrays.\n",
    "        x = torch.from_numpy(x).to(torch.int64)\n",
    "        y = torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "        return TensorDataset(x, y)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # load data\n",
    "        train_x, test_x, train_y, test_y = self.load_data('./data/spam_data.pkl')\n",
    "        # fit tokenizer\n",
    "        self.tok.fit_on_texts(train_x)\n",
    "        # make datasets\n",
    "        self.train = self.make_dataset(train_x, train_y)\n",
    "        self.test = self.make_dataset(train_x, train_y)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=mp.cpu_count() // 4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=mp.cpu_count() // 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44c40fb81d023f68aad26129d8e12ca3",
     "grade": false,
     "grade_id": "cell-317b71960ee93b6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 1 [15 points]\n",
    "\n",
    "Complete the functions `init`, `training_step`, and `validation_step` and `forward`functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "842b9172075d5ee7fbcf051d8ae8b7c7",
     "grade": false,
     "grade_id": "cell-2673775575f12b54",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class RNN(pl.LightningModule):\n",
    "    \"\"\" RNN classifier \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_length=32, dict_size=5000, example_length=150):\n",
    "        \"\"\"\n",
    "        initialize RNN model\n",
    "        :param embedding_length: size of word embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: build model with Embedding, LSTM and dense layers.\n",
    "        # Documentation for LSTM layer in :\n",
    "        #     https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        # Note there are multiple ways to implement your model, \n",
    "        # we suggest adding individual layers here, but any method is fine.\n",
    "\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        self.example_input_array = torch.zeros([1, 150], dtype=torch.int64)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Implement a forward call your model here.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return out\n",
    "\n",
    "    def accuracy(self, y_hat, y):\n",
    "        return (y == y_hat.round()).to(torch.float32).mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\" Perfom a training step.\n",
    "        \n",
    "        TODO:\n",
    "            - forward pass on data in batch\n",
    "            - compute training loss.\n",
    "            - Compute training accuracy \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # We implemented logging for you. \n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log('train_loss', loss)\n",
    "        result.log('train_accuracy', acc, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\" Perfom a test step \n",
    "            hint: your code should be the same as your train step\n",
    "        \n",
    "        TODO:\n",
    "            - forward pass on data in batch\n",
    "            - compute test loss.\n",
    "            - Compute test accuracy  \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # We implemented logging for you. \n",
    "        result = pl.EvalResult(loss)\n",
    "        result.log('test_loss', loss, prog_bar=True)\n",
    "        result.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data \n",
    "spam_ham_dm = SpamHam()\n",
    "\n",
    "def run_rnn(**kwargs):\n",
    "    # helper function for running RNN.\n",
    "    logger = CSVLogger(\"logs\", name=\"rnn\")\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=int(torch.cuda.is_available()),\n",
    "        logger=logger,\n",
    "        min_epochs=5,\n",
    "        max_epochs=5,\n",
    "        row_log_interval=1,\n",
    "        log_save_interval=1\n",
    "    )\n",
    "    \n",
    "    model = RNN(**kwargs)\n",
    "    trainer.fit(model, spam_ham_dm)\n",
    "    results = trainer.test()\n",
    "    return results, logger.experiment.metrics_file_path\n",
    "\n",
    "results, _ = run_rnn()\n",
    "print('Accuracy for LSTM: ', results[0]['test_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2ccf7f440d3189f35e9610eacb0c3bec",
     "grade": true,
     "grade_id": "cell-41f4dc66c60e24b2",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f26e2f2c38d1ddd3c8d5cca106f4f31a",
     "grade": false,
     "grade_id": "cell-41cfba637db6a1a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Part 2 [8 points]\n",
    "* Change the embedding length and experiment with these values: [8, 16, 32, 48, 64].\n",
    "* Plot training accuracies of different embedding lengths vs. epochs.\n",
    "* Observe and explain the impact of embedding length in LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfiles = []\n",
    "test_accuracy_array = []\n",
    "\n",
    "for embedding_len in [8, 16, 32, 48, 64]:    \n",
    "    results, metrics = run_rnn(embedding_length=embedding_len)\n",
    "    print('Accuracy for LSTM: ', results[0]['test_acc'])\n",
    "    mfiles.append(metrics)\n",
    "\n",
    "train_accuracy_matrix = np.array(\n",
    "    [pd.read_csv(f)[\"train_accuracy\"].values for f in mfiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70841200f15334e929c905c548786aba",
     "grade": false,
     "grade_id": "cell-c89ec93459d7f684",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3452a101b4fa0405580e01fd3227a774",
     "grade": true,
     "grade_id": "cell-bbb3e96b84261447",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e65b1d3e3766e5df4b3f17332ca8fa0b",
     "grade": false,
     "grade_id": "cell-6edf99edb9a7bdcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Give your observation and explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "80155864567eb1ff7ebeafa96eaa8585",
     "grade": true,
     "grade_id": "cell-b621817b4445c626",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "340px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
